{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import train_multi_rnn\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from spike_train_visual import *\n",
    "from read_spike_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60  9 19 70 55 61 71 13 75 30 44 43 84 83 66 80 40 26  1 42 63 41 50 78 74\n",
      " 18 46 69  3 58  5 51 31 59 67  4 15 29 54 68 36 17 37 14 62  8 23 38 49 65\n",
      " 22 21 76  6 48 39  7 47 56 10 45 79 64 52 35 25 28  0 81 72 20 34 53 82 33\n",
      " 77 32 24 16 11 12 27 73  2 57]\n"
     ]
    }
   ],
   "source": [
    "#orders=np.arange(85)\n",
    "#np.random.shuffle(orders)\n",
    "#print ','.join(map(str,list(orders)))\n",
    "orders=np.array([60,9,19,70,55,61,71,13,75,30,44,43,84,83,66,80,40,26,1,42,63,41,50,78,74,18,46,69,3,58,5,51,31,59,67,4,15,29,54,68,36,17,37,14,62,8,23,38,49,65,22,21,76,6,48,39,7,47,56,10,45,79,64,52,35,25,28,0,81,72,20,34,53,82,33,77,32,24,16,11,12,27,73,2,57])\n",
    "print orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=np.genfromtxt('/home/honglei/projects/neural_network/data/small/truncated_23_0001.csv', delimiter=',')\n",
    "# #print data[0:5,orders]\n",
    "# #data[:,[6,12]]=data[:,[12,6]]\n",
    "# np.savetxt('/home/honglei/projects/neural_network/data/shuffle/truncated_23_0001.csv',data[:,orders],fmt='%d',delimiter=',')\n",
    "\n",
    "# data1=np.genfromtxt('/home/honglei/projects/neural_network/data/small/truncated_23_0001.csv', delimiter=',')\n",
    "# data2=np.genfromtxt('/home/honglei/projects/neural_network/data/shuffle/truncated_23_0001.csv', delimiter=',')\n",
    "# print np.linalg.norm(data1[:,orders]-data2)\n",
    "# print np.linalg.norm(data1[:,9]-data2[:,1])\n",
    "\n",
    "\n",
    "np.savetxt('/home/honglei/projects/neural_network/data/single_ch/6_alone.csv',data[:,12],fmt='%d',delimiter=',')\n",
    "\n",
    "# data=np.genfromtxt('/home/honglei/projects/neural_network/data/I1923_0001.csv', delimiter=',')\n",
    "# np.savetxt('/home/honglei/projects/neural_network/data/small1/truncated_23_0001.csv',data[0:10000,],fmt='%d',delimiter=',')\n",
    "# data=np.genfromtxt('/home/honglei/projects/neural_network/data/I1921_0001.csv', delimiter=',')\n",
    "# np.savetxt('/home/honglei/projects/neural_network/data/small1/truncated_21_0001.csv',data[0:10000,],fmt='%d',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "(8000, 85)\n",
      "Building model\n",
      "False\n",
      "model/shared_variables/Cell0\n",
      "Training variables: [u'model/shuffle.csv/input_w:0', u'model/shared_variables/Cell0/Linear/Matrix:0', u'model/shared_variables/Cell0/Linear/Bias:0']\n",
      "True\n",
      "model/shared_variables/Cell0\n",
      "Training variables: [u'model/shuffle.csv/input_w:0', u'model/shared_variables/Cell0/Linear/Matrix:0', u'model/shared_variables/Cell0/Linear/Bias:0', u'model/truncated_23_0001.csv/input_w:0']\n",
      "Shared variables: [u'model/shared_variables/Cell0/Linear/Matrix:0', u'model/shared_variables/Cell0/Linear/Bias:0']\n",
      "Starting session\n",
      "model/shuffle.csv/input_w:0\n",
      "model/shared_variables/Cell0/Linear/Matrix:0\n",
      "model/shared_variables/Cell0/Linear/Bias:0\n",
      "model/shuffle.csv/model/shuffle.csv/input_w/RMSProp:0\n",
      "model/shuffle.csv/model/shuffle.csv/input_w/RMSProp_1:0\n",
      "model/shuffle.csv/model/shared_variables/Cell0/Linear/Matrix/RMSProp:0\n",
      "model/shuffle.csv/model/shared_variables/Cell0/Linear/Matrix/RMSProp_1:0\n",
      "model/shuffle.csv/model/shared_variables/Cell0/Linear/Bias/RMSProp:0\n",
      "model/shuffle.csv/model/shared_variables/Cell0/Linear/Bias/RMSProp_1:0\n",
      "model/truncated_23_0001.csv/input_w:0\n",
      "model/truncated_23_0001.csv/model/shared_variables/Cell0/Linear/Matrix/RMSProp:0\n",
      "model/truncated_23_0001.csv/model/shared_variables/Cell0/Linear/Matrix/RMSProp_1:0\n",
      "model/truncated_23_0001.csv/model/shared_variables/Cell0/Linear/Bias/RMSProp:0\n",
      "model/truncated_23_0001.csv/model/shared_variables/Cell0/Linear/Bias/RMSProp_1:0\n",
      "model/truncated_23_0001.csv/model/truncated_23_0001.csv/input_w/RMSProp:0\n",
      "model/truncated_23_0001.csv/model/truncated_23_0001.csv/input_w/RMSProp_1:0\n"
     ]
    }
   ],
   "source": [
    "train_multi_rnn.FLAGS.log_dir='/home/honglei/projects/neural_network/log_dir'\n",
    "m=train_multi_rnn.TrainMultiRNN('/home/honglei/projects/neural_network/data/shuffle_small//', fix_shared=False)\n",
    "#m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "(8000, 85)\n",
      "Building model\n",
      "False\n",
      "model/shared_variables/Cell0\n",
      "Training variables: [u'model/truncated_23_0001.csv/input_w:0']\n",
      "Shared variables: [u'model/shared_variables/Cell0/Linear/Matrix:0', u'model/shared_variables/Cell0/Linear/Bias:0']\n",
      "Starting session\n",
      "model/truncated_23_0001.csv/input_w:0\n",
      "model/shared_variables/Cell0/Linear/Matrix:0\n",
      "model/shared_variables/Cell0/Linear/Bias:0\n",
      "model/truncated_23_0001.csv/model/truncated_23_0001.csv/input_w/RMSProp:0\n",
      "model/truncated_23_0001.csv/model/truncated_23_0001.csv/input_w/RMSProp_1:0\n"
     ]
    }
   ],
   "source": [
    "train_multi_rnn.FLAGS.log_dir='/home/honglei/projects/neural_network/log_dir'\n",
    "m=train_multi_rnn.TrainMultiRNN('/home/honglei/projects/neural_network/data/shuffle', fix_shared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.699 speed: 7 batches/sec\n",
      "8/80 loss: 0.695 speed: 12 batches/sec\n",
      "16/80 loss: 0.691 speed: 12 batches/sec\n",
      "24/80 loss: 0.684 speed: 13 batches/sec\n",
      "32/80 loss: 0.676 speed: 13 batches/sec\n",
      "40/80 loss: 0.665 speed: 13 batches/sec\n",
      "48/80 loss: 0.651 speed: 13 batches/sec\n",
      "56/80 loss: 0.634 speed: 13 batches/sec\n",
      "64/80 loss: 0.611 speed: 13 batches/sec\n",
      "72/80 loss: 0.586 speed: 13 batches/sec\n",
      "Epoch: 1 Train Loss: 0.564\n",
      "Epoch: 1 Train ROC-AUC: 0.516, PR-AUC: 0.003\n",
      "Saving latest results.\n",
      "Epoch: 1 Valid ROC-AUC: 0.550, PR-AUC: 0.004\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.285 speed: 14 batches/sec\n",
      "8/80 loss: 0.226 speed: 14 batches/sec\n",
      "16/80 loss: 0.196 speed: 13 batches/sec\n",
      "24/80 loss: 0.170 speed: 13 batches/sec\n",
      "32/80 loss: 0.146 speed: 13 batches/sec\n",
      "40/80 loss: 0.127 speed: 13 batches/sec\n",
      "48/80 loss: 0.113 speed: 13 batches/sec\n",
      "56/80 loss: 0.101 speed: 13 batches/sec\n",
      "64/80 loss: 0.091 speed: 13 batches/sec\n",
      "72/80 loss: 0.085 speed: 13 batches/sec\n",
      "Epoch: 2 Train Loss: 0.079\n",
      "Epoch: 2 Train ROC-AUC: 0.703, PR-AUC: 0.012\n",
      "Saving latest results.\n",
      "Epoch: 2 Valid ROC-AUC: 0.716, PR-AUC: 0.029\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.024 speed: 14 batches/sec\n",
      "8/80 loss: 0.015 speed: 13 batches/sec\n",
      "16/80 loss: 0.014 speed: 13 batches/sec\n",
      "24/80 loss: 0.016 speed: 13 batches/sec\n",
      "32/80 loss: 0.016 speed: 13 batches/sec\n",
      "40/80 loss: 0.014 speed: 13 batches/sec\n",
      "48/80 loss: 0.015 speed: 13 batches/sec\n",
      "56/80 loss: 0.015 speed: 13 batches/sec\n",
      "64/80 loss: 0.014 speed: 13 batches/sec\n",
      "72/80 loss: 0.015 speed: 13 batches/sec\n",
      "Epoch: 3 Train Loss: 0.015\n",
      "Epoch: 3 Train ROC-AUC: 0.890, PR-AUC: 0.030\n",
      "Saving latest results.\n",
      "Epoch: 3 Valid ROC-AUC: 0.864, PR-AUC: 0.029\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.021 speed: 14 batches/sec\n",
      "8/80 loss: 0.013 speed: 13 batches/sec\n",
      "16/80 loss: 0.011 speed: 13 batches/sec\n",
      "24/80 loss: 0.013 speed: 13 batches/sec\n",
      "32/80 loss: 0.014 speed: 13 batches/sec\n",
      "40/80 loss: 0.012 speed: 13 batches/sec\n",
      "48/80 loss: 0.013 speed: 13 batches/sec\n",
      "56/80 loss: 0.013 speed: 13 batches/sec\n",
      "64/80 loss: 0.012 speed: 13 batches/sec\n",
      "72/80 loss: 0.014 speed: 13 batches/sec\n",
      "Epoch: 4 Train Loss: 0.014\n",
      "Epoch: 4 Train ROC-AUC: 0.912, PR-AUC: 0.034\n",
      "Saving latest results.\n",
      "Epoch: 4 Valid ROC-AUC: 0.886, PR-AUC: 0.030\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.020 speed: 14 batches/sec\n",
      "8/80 loss: 0.012 speed: 13 batches/sec\n",
      "16/80 loss: 0.010 speed: 13 batches/sec\n",
      "24/80 loss: 0.013 speed: 13 batches/sec\n",
      "32/80 loss: 0.013 speed: 13 batches/sec\n",
      "40/80 loss: 0.012 speed: 13 batches/sec\n",
      "48/80 loss: 0.012 speed: 13 batches/sec\n",
      "56/80 loss: 0.013 speed: 13 batches/sec\n",
      "64/80 loss: 0.012 speed: 13 batches/sec\n",
      "72/80 loss: 0.013 speed: 13 batches/sec\n",
      "Epoch: 5 Train Loss: 0.013\n",
      "Epoch: 5 Train ROC-AUC: 0.925, PR-AUC: 0.044\n",
      "Saving latest results.\n",
      "Epoch: 5 Valid ROC-AUC: 0.892, PR-AUC: 0.038\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.021 speed: 14 batches/sec\n",
      "8/80 loss: 0.012 speed: 13 batches/sec\n",
      "16/80 loss: 0.010 speed: 13 batches/sec\n",
      "24/80 loss: 0.012 speed: 13 batches/sec\n",
      "32/80 loss: 0.013 speed: 13 batches/sec\n",
      "40/80 loss: 0.012 speed: 13 batches/sec\n",
      "48/80 loss: 0.012 speed: 13 batches/sec\n",
      "56/80 loss: 0.013 speed: 13 batches/sec\n",
      "64/80 loss: 0.012 speed: 13 batches/sec\n",
      "72/80 loss: 0.013 speed: 13 batches/sec\n",
      "Epoch: 6 Train Loss: 0.013\n",
      "Epoch: 6 Train ROC-AUC: 0.931, PR-AUC: 0.049\n",
      "Saving latest results.\n",
      "Epoch: 6 Valid ROC-AUC: 0.887, PR-AUC: 0.028\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.020 speed: 14 batches/sec\n",
      "8/80 loss: 0.012 speed: 13 batches/sec\n",
      "16/80 loss: 0.010 speed: 13 batches/sec\n",
      "24/80 loss: 0.012 speed: 13 batches/sec\n",
      "32/80 loss: 0.013 speed: 13 batches/sec\n",
      "40/80 loss: 0.011 speed: 13 batches/sec\n",
      "48/80 loss: 0.012 speed: 13 batches/sec\n",
      "56/80 loss: 0.012 speed: 13 batches/sec\n",
      "64/80 loss: 0.012 speed: 13 batches/sec\n",
      "72/80 loss: 0.013 speed: 13 batches/sec\n",
      "Epoch: 7 Train Loss: 0.013\n",
      "Epoch: 7 Train ROC-AUC: 0.936, PR-AUC: 0.054\n",
      "Saving latest results.\n",
      "Epoch: 7 Valid ROC-AUC: 0.883, PR-AUC: 0.029\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.020 speed: 14 batches/sec\n",
      "8/80 loss: 0.012 speed: 13 batches/sec\n",
      "16/80 loss: 0.010 speed: 13 batches/sec\n",
      "24/80 loss: 0.012 speed: 13 batches/sec\n",
      "32/80 loss: 0.013 speed: 13 batches/sec\n",
      "40/80 loss: 0.011 speed: 13 batches/sec\n",
      "48/80 loss: 0.011 speed: 13 batches/sec\n",
      "56/80 loss: 0.012 speed: 13 batches/sec\n",
      "64/80 loss: 0.011 speed: 13 batches/sec\n",
      "72/80 loss: 0.013 speed: 13 batches/sec\n",
      "Epoch: 8 Train Loss: 0.012\n",
      "Epoch: 8 Train ROC-AUC: 0.932, PR-AUC: 0.057\n",
      "Epoch: 8 Valid ROC-AUC: 0.868, PR-AUC: 0.030\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.020 speed: 14 batches/sec\n",
      "8/80 loss: 0.011 speed: 13 batches/sec\n",
      "16/80 loss: 0.010 speed: 13 batches/sec\n",
      "24/80 loss: 0.012 speed: 13 batches/sec\n",
      "32/80 loss: 0.012 speed: 13 batches/sec\n",
      "40/80 loss: 0.011 speed: 13 batches/sec\n",
      "48/80 loss: 0.011 speed: 13 batches/sec\n",
      "56/80 loss: 0.012 speed: 13 batches/sec\n",
      "64/80 loss: 0.011 speed: 13 batches/sec\n",
      "72/80 loss: 0.012 speed: 13 batches/sec\n",
      "Epoch: 9 Train Loss: 0.012\n",
      "Epoch: 9 Train ROC-AUC: 0.945, PR-AUC: 0.068\n",
      "Saving latest results.\n",
      "Epoch: 9 Valid ROC-AUC: 0.876, PR-AUC: 0.031\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.020 speed: 14 batches/sec\n",
      "8/80 loss: 0.011 speed: 13 batches/sec\n",
      "16/80 loss: 0.010 speed: 13 batches/sec\n",
      "24/80 loss: 0.012 speed: 13 batches/sec\n",
      "32/80 loss: 0.012 speed: 13 batches/sec\n",
      "40/80 loss: 0.011 speed: 13 batches/sec\n",
      "48/80 loss: 0.011 speed: 13 batches/sec\n",
      "56/80 loss: 0.012 speed: 13 batches/sec\n",
      "64/80 loss: 0.011 speed: 13 batches/sec\n",
      "72/80 loss: 0.012 speed: 13 batches/sec\n",
      "Epoch: 10 Train Loss: 0.012\n",
      "Epoch: 10 Train ROC-AUC: 0.948, PR-AUC: 0.071\n",
      "Saving latest results.\n",
      "Epoch: 10 Valid ROC-AUC: 0.876, PR-AUC: 0.027\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.019 speed: 14 batches/sec\n",
      "8/80 loss: 0.011 speed: 13 batches/sec\n",
      "16/80 loss: 0.009 speed: 13 batches/sec\n",
      "24/80 loss: 0.012 speed: 13 batches/sec\n",
      "32/80 loss: 0.012 speed: 13 batches/sec\n",
      "40/80 loss: 0.011 speed: 13 batches/sec\n",
      "48/80 loss: 0.011 speed: 13 batches/sec\n",
      "56/80 loss: 0.012 speed: 13 batches/sec\n",
      "64/80 loss: 0.011 speed: 13 batches/sec\n",
      "72/80 loss: 0.012 speed: 13 batches/sec\n",
      "Epoch: 11 Train Loss: 0.012\n",
      "Epoch: 11 Train ROC-AUC: 0.946, PR-AUC: 0.073\n",
      "Epoch: 11 Valid ROC-AUC: 0.871, PR-AUC: 0.022\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.020 speed: 14 batches/sec\n",
      "8/80 loss: 0.011 speed: 13 batches/sec\n",
      "16/80 loss: 0.009 speed: 13 batches/sec\n",
      "24/80 loss: 0.011 speed: 13 batches/sec\n",
      "32/80 loss: 0.012 speed: 13 batches/sec\n",
      "40/80 loss: 0.011 speed: 13 batches/sec\n",
      "48/80 loss: 0.011 speed: 13 batches/sec\n",
      "56/80 loss: 0.012 speed: 13 batches/sec\n",
      "64/80 loss: 0.011 speed: 13 batches/sec\n",
      "72/80 loss: 0.012 speed: 13 batches/sec\n",
      "Epoch: 12 Train Loss: 0.012\n",
      "Epoch: 12 Train ROC-AUC: 0.952, PR-AUC: 0.087\n",
      "Saving latest results.\n",
      "Epoch: 12 Valid ROC-AUC: 0.868, PR-AUC: 0.025\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.019 speed: 12 batches/sec\n",
      "8/80 loss: 0.011 speed: 12 batches/sec\n",
      "16/80 loss: 0.009 speed: 13 batches/sec\n",
      "24/80 loss: 0.011 speed: 13 batches/sec\n",
      "32/80 loss: 0.012 speed: 13 batches/sec\n",
      "40/80 loss: 0.011 speed: 13 batches/sec\n",
      "48/80 loss: 0.011 speed: 13 batches/sec\n",
      "56/80 loss: 0.011 speed: 13 batches/sec\n",
      "64/80 loss: 0.011 speed: 13 batches/sec\n",
      "72/80 loss: 0.012 speed: 13 batches/sec\n",
      "Epoch: 13 Train Loss: 0.012\n",
      "Epoch: 13 Train ROC-AUC: 0.955, PR-AUC: 0.085\n",
      "Saving latest results.\n",
      "Epoch: 13 Valid ROC-AUC: 0.867, PR-AUC: 0.028\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.018 speed: 14 batches/sec\n",
      "8/80 loss: 0.011 speed: 13 batches/sec\n",
      "16/80 loss: 0.009 speed: 13 batches/sec\n",
      "24/80 loss: 0.011 speed: 13 batches/sec\n",
      "32/80 loss: 0.012 speed: 13 batches/sec\n",
      "40/80 loss: 0.010 speed: 13 batches/sec\n",
      "48/80 loss: 0.011 speed: 13 batches/sec\n",
      "56/80 loss: 0.011 speed: 13 batches/sec\n",
      "64/80 loss: 0.011 speed: 13 batches/sec\n",
      "72/80 loss: 0.012 speed: 13 batches/sec\n",
      "Epoch: 14 Train Loss: 0.012\n",
      "Epoch: 14 Train ROC-AUC: 0.954, PR-AUC: 0.090\n",
      "Epoch: 14 Valid ROC-AUC: 0.869, PR-AUC: 0.026\n",
      "8000\n",
      "100\n",
      "80\n",
      "0/80 loss: 0.018 speed: 13 batches/sec\n",
      "8/80 loss: 0.011 speed: 13 batches/sec\n",
      "16/80 loss: 0.009 speed: 13 batches/sec\n",
      "24/80 loss: 0.011 speed: 13 batches/sec\n",
      "32/80 loss: 0.012 speed: 13 batches/sec\n",
      "40/80 loss: 0.010 speed: 13 batches/sec\n",
      "48/80 loss: 0.011 speed: 13 batches/sec\n",
      "56/80 loss: 0.011 speed: 13 batches/sec\n",
      "64/80 loss: 0.011 speed: 13 batches/sec\n",
      "WARNING: User interrupted program.\n",
      "Do you want to save the latest data? [y/n]n\n",
      "Results deleted.\n"
     ]
    }
   ],
   "source": [
    "m.partially_train('/home/honglei/projects/neural_network/log_dir/test_small.ckt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189.01253, 189.10707, 186.18031, 188.3015, 185.95229, 186.07175, 187.41185, 189.49435, 190.12299, 187.86702, 185.88139, 184.6797, 185.27435, 188.62473, 186.93918, 187.25377, 191.1053, 184.9077, 188.96356, 190.37833, 187.09399, 189.00517, 185.9955, 185.22289, 183.06477, 188.20303, 187.5865, 188.41893, 185.93799, 186.86914, 185.96077, 188.03783, 185.57463, 185.58244, 186.73927, 185.82025, 187.6485, 188.54814, 187.71747, 184.41495, 185.69759, 189.38234, 186.68858, 184.88885, 186.71269, 188.66484, 185.88707, 188.01375, 186.65341, 187.98712]\n"
     ]
    }
   ],
   "source": [
    "m.restore('/home/honglei/projects/neural_network/log_dir/test_small.ckt')\n",
    "origin_inputw=m.get_value_of_variable('model/truncated_23_0001.csv/input_w:0')\n",
    "m.restore('/home/honglei/projects/neural_network/log_dir/test_shuffle.ckt')\n",
    "shuffle_inputw=m.get_value_of_variable('model/truncated_23_0001.csv/input_w:0')\n",
    "d2=[]\n",
    "for i in range(50):\n",
    "    rand_orders=np.arange(85)\n",
    "    np.random.shuffle(rand_orders)\n",
    "    rand_inputw=shuffle_inputw[rand_orders,:]\n",
    "    d2.append(np.linalg.norm(origin_inputw-rand_inputw))\n",
    "print d2\n",
    "d1=[np.linalg.norm(origin_inputw-shuffle_inputw)]*len(d2)\n",
    "d3=[np.linalg.norm(origin_inputw[orders,:]-shuffle_inputw)]*len(d2)\n",
    "plt.boxplot([d1,d2,d3],0,'')\n",
    "plt.xticks([1,2,3], ['w1-w2', 'w1-shuffled_w2','w1-reordered_w2'])\n",
    "plt.ylabel('L2 norm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1 63  1 63  1  1 63  1 63  1 63 23  1 32 63 63  1  1  1 63 63 70  1  1\n",
      "  1  1  1 63 63  1  1 34 63 53  1  1  1  1 34  1 63  1 12 63  1 63  1 63  1\n",
      "  1  1 63  1 12  1  1  1 32 63 63  1 63 12  1 63 63 63 63  1 63  1 63 63  1\n",
      " 29 63  1  1 63  1 63  1  1 63]\n",
      "[51 51 51 51 51 51  9 51 51  6 51 51 50 51 51 51 51 51 51 51 51 51 70 51 51\n",
      " 51 51 51 51 51 51 51 58 51 39 51 51 51 51 51 51 51 51 25 51 51 51 51 51 51\n",
      " 51 51 51 51 76 51 51 51 51 51 51 51 51 48 51 51 51 51 51 51 22 51 51 51 51\n",
      " 51 51 51 51 51 51 83 51 51 51]\n",
      "[60  9 19 70 55 61 71 13 75 30 44 43 84 83 66 80 40 26  1 42 63 41 50 78 74\n",
      " 18 46 69  3 58  5 51 31 59 67  4 15 29 54 68 36 17 37 14 62  8 23 38 49 65\n",
      " 22 21 76  6 48 39  7 47 56 10 45 79 64 52 35 25 28  0 81 72 20 34 53 82 33\n",
      " 77 32 24 16 11 12 27 73  2 57]\n",
      "[ 5717.49939387  5717.49999987  5714.45097476  5717.49999979  5704.62341391\n",
      "  5717.49999972  5699.15142744  5714.96120606  5717.49999277  5677.0996478\n",
      "  5717.47118197  5709.98912313  5672.92303918  5717.49999152  5690.72669772\n",
      "  5713.13114988  5698.91025572  5717.49999964  5717.49999971  5717.49999982\n",
      "  5714.49409447  5713.23257004  5671.67282572  5716.28529361  5717.49999982\n",
      "  5716.54321316  5717.49881708  5712.3764277   5715.27048582  5703.1632445\n",
      "  5717.41115118  5717.49999952  5555.47130928  5715.52121746  5666.2047946\n",
      "  5672.63837403  5717.49769928  5717.49999975  5717.49996524  5639.33673564\n",
      "  5717.49895326  5714.59594196  5717.49999974  5125.3502798   5704.46672844\n",
      "  5717.49786472  5681.60519744  5717.49258163  5715.49999168  5710.62538614\n",
      "  5717.49979066  5717.49999988  5711.21974917  5667.86889904  5142.93865021\n",
      "  5717.49999981  5717.49999981  5717.20314208  5703.45591708  5715.42653489\n",
      "  5715.49998298  5717.49999926  5715.47743414  4533.8406936   5715.60603982\n",
      "  5715.5929434   5715.49762778  5711.12671624  5715.50955218  5717.4950143\n",
      "  5698.25673584  5717.4999997   5713.38860441  5711.49828491  5717.49999974\n",
      "  5272.95266152  5705.39078503  5717.49999982  5717.49999978  5715.24531612\n",
      "  5717.49999973  5710.37292277  5717.49999978  5668.5463402   5715.62715507]\n",
      "[   1.    0.   28.    0.   29.    1.   87.   15.   11.   36.    9.    7.\n",
      "  280.    7.   82.    5.  151.    0.    0.    0.   29.    6.   32.   29.\n",
      "    0.   23.    4.    7.    9.  111.    2.    0.   21.   10.  116.    4.\n",
      "   11.    0.    3.   53.    3.   21.    0.    2.   30.    1.    9.    2.\n",
      "    5.    2.    1.    0.   76.   14.   16.    0.    0.   14.   29.    2.\n",
      "   43.    0.   17.    2.    3.   16.   17.   46.    6.    8.   69.    1.\n",
      "   24.   68.    0.    6.   48.    0.    0.   30.    0.   80.    0.    3.\n",
      "    9.]\n"
     ]
    }
   ],
   "source": [
    "m.restore('/home/honglei/projects/neural_network/log_dir/test_shuffle_small.ckt')\n",
    "origin_inputw=m.get_value_of_variable('model/truncated_23_0001.csv/input_w:0')\n",
    "shuffle_inputw=m.get_value_of_variable('model/shuffle.csv/input_w:0')\n",
    "# d2=[]\n",
    "# for i in range(50):\n",
    "#     rand_orders=np.arange(85)\n",
    "#     np.random.shuffle(rand_orders)\n",
    "#     rand_inputw=shuffle_inputw[rand_orders,:]\n",
    "#     d2.append(np.linalg.norm(origin_inputw-rand_inputw))\n",
    "# print d2\n",
    "# d1=[np.linalg.norm(origin_inputw-shuffle_inputw)]*len(d2)\n",
    "# d3=[np.linalg.norm(origin_inputw[orders,:]-shuffle_inputw)]*len(d2)\n",
    "# plt.boxplot([d1,d2,d3],0,'')\n",
    "# plt.xticks([1,2,3], ['w1-w2', 'w1-shuffled_w2','w1-reordered_w2'])\n",
    "# plt.ylabel('L2 norm')\n",
    "# plt.show()\n",
    "\n",
    "data1=np.genfromtxt('/home/honglei/projects/neural_network/data/shuffle_small//truncated_23_0001.csv', delimiter=',')\n",
    "data2=np.genfromtxt('/home/honglei/projects/neural_network/data/shuffle_small/shuffle.csv', delimiter=',')\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "rep_data1=np.tanh(np.dot(data1,origin_inputw))\n",
    "# rep_data2=np.dot(data2,shuffle_inputw)\n",
    "# print np.linalg.norm(rep_data1-rep_data2)\n",
    "# visualSeries(rep_data1)\n",
    "# visualSeries(rep_data2)\n",
    "\n",
    "mapped_data1=sigmoid(np.dot(rep_data1,np.transpose(origin_inputw)))\n",
    "norms=np.zeros([85,85])\n",
    "for i in range(85):\n",
    "    for j in range(85):\n",
    "        norms[i,j]=np.linalg.norm(data1[:,i]-mapped_data1[:,j])\n",
    "print np.argmax(norms,axis=0)\n",
    "print np.argmax(norms,axis=1)\n",
    "print orders\n",
    "print np.sum(mapped_data1,axis=0)\n",
    "print np.sum(data1,axis=0)\n",
    "#visualSeries(mapped_data1)\n",
    "#visualSeries(data1)\n",
    "plt.pcolor(shuffle_inputw)\n",
    "plt.show()\n",
    "\n",
    "# #trans_w=np.dot(origin_inputw,np.transpose(shuffle_inputw))\n",
    "# trans_w=np.dot(shuffle_inputw,np.transpose(shuffle_inputw))\n",
    "# mapping=np.argmax(trans_w,axis=0)\n",
    "# print mapping\n",
    "# print orders\n",
    "# np.sum(mapping==orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1109.65516248\n"
     ]
    }
   ],
   "source": [
    "m.restore('/home/honglei/projects/neural_network/log_dir/test_two_small.ckt')\n",
    "inputw1=m.get_value_of_variable('model/truncated_23_0001.csv/input_w:0')\n",
    "inputw2=m.get_value_of_variable('model/truncated_21_0001.csv/input_w:0')\n",
    "data1=np.genfromtxt('/home/honglei/projects/neural_network/data/small1/truncated_23_0001.csv', delimiter=',')\n",
    "data2=np.genfromtxt('/home/honglei/projects/neural_network/data/small1/truncated_21_0001.csv', delimiter=',')\n",
    "\n",
    "rep_data1=np.dot(data1,inputw1)\n",
    "rep_data2=np.dot(data2,inputw2)\n",
    "\n",
    "#print np.linalg.norm(data1-data2)\n",
    "print np.linalg.norm(rep_data1-rep_data2)\n",
    "visualSeries(data1)\n",
    "visualSeries(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "(8000, 85)\n",
      "Building model\n",
      "Training variables: [u'model/shuffle.csv/Layer1/weights:0', u'model/shared_variables/Layer2/weights:0', u'model/shared_variables/Layer2/bias:0']\n",
      "Training variables: [u'model/shuffle.csv/Layer1/weights:0', u'model/shared_variables/Layer2/weights:0', u'model/shared_variables/Layer2/bias:0', u'model/truncated_23_0001.csv/Layer1/weights:0']\n",
      "Shared variables: [u'model/shared_variables/Layer2/weights:0', u'model/shared_variables/Layer2/bias:0']\n",
      "Starting session\n",
      "model/shuffle.csv/Layer1/weights:0\n",
      "model/shared_variables/Layer2/weights:0\n",
      "model/shared_variables/Layer2/bias:0\n",
      "model/shuffle.csv/model/shuffle.csv/Layer1/weights/RMSProp:0\n",
      "model/shuffle.csv/model/shuffle.csv/Layer1/weights/RMSProp_1:0\n",
      "model/shuffle.csv/model/shared_variables/Layer2/weights/RMSProp:0\n",
      "model/shuffle.csv/model/shared_variables/Layer2/weights/RMSProp_1:0\n",
      "model/shuffle.csv/model/shared_variables/Layer2/bias/RMSProp:0\n",
      "model/shuffle.csv/model/shared_variables/Layer2/bias/RMSProp_1:0\n",
      "model/truncated_23_0001.csv/Layer1/weights:0\n",
      "model/truncated_23_0001.csv/model/shared_variables/Layer2/weights/RMSProp:0\n",
      "model/truncated_23_0001.csv/model/shared_variables/Layer2/weights/RMSProp_1:0\n",
      "model/truncated_23_0001.csv/model/shared_variables/Layer2/bias/RMSProp:0\n",
      "model/truncated_23_0001.csv/model/shared_variables/Layer2/bias/RMSProp_1:0\n",
      "model/truncated_23_0001.csv/model/truncated_23_0001.csv/Layer1/weights/RMSProp:0\n",
      "model/truncated_23_0001.csv/model/truncated_23_0001.csv/Layer1/weights/RMSProp_1:0\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import train_multi_fcnet\n",
    "\n",
    "train_multi_fcnet.FLAGS.log_dir='/home/honglei/projects/neural_network/log_dir'\n",
    "m=train_multi_fcnet.TrainMultiFCNet('/home/honglei/projects/neural_network/data/shuffle_small/', fix_shared=False)\n",
    "#m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120.48477, 120.91798, 120.91375, 120.66364, 120.56027, 120.44719, 119.93005, 120.56586, 120.1909, 120.61044, 120.16321, 120.75238, 120.73853, 120.33401, 120.59312, 120.66002, 120.09286, 120.44724, 119.72305, 119.68443, 120.0192, 120.33186, 120.55452, 120.34799, 119.57524, 120.29719, 119.6647, 120.32313, 120.37698, 120.76255, 120.32048, 120.09824, 119.69699, 120.46315, 120.78207, 120.43684, 120.16575, 119.87554, 119.9917, 120.5054, 120.62099, 119.19208, 119.86597, 119.56715, 120.46431, 120.2859, 120.36205, 120.1976, 119.80991, 120.40553]\n"
     ]
    }
   ],
   "source": [
    "m.restore('/home/honglei/projects/neural_network/log_dir/fcnet_small_shuffle.ckt')\n",
    "origin_inputw=m.get_value_of_variable('model/truncated_23_0001.csv/Layer1/weights:0')\n",
    "shuffle_inputw=m.get_value_of_variable('model/shuffle.csv/Layer1/weights:0')\n",
    "d2=[]\n",
    "for i in range(50):\n",
    "    rand_orders=np.arange(85)\n",
    "    np.random.shuffle(rand_orders)\n",
    "    rand_inputw=shuffle_inputw[rand_orders,:]\n",
    "    d2.append(np.linalg.norm(origin_inputw-rand_inputw))\n",
    "print d2\n",
    "d1=[np.linalg.norm(origin_inputw-shuffle_inputw)]*len(d2)\n",
    "d3=[np.linalg.norm(origin_inputw[orders,:]-shuffle_inputw)]*len(d2)\n",
    "plt.boxplot([d1,d2,d3],0,'')\n",
    "plt.xticks([1,2,3], ['w1-w2', 'w1-shuffled_w2','w1-reordered_w2'])\n",
    "plt.ylabel('L2 norm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#m.restore('/home/honglei/projects/neural_network/log_dir/best.ckt')\n",
    "X_list, predict_list=m.get_train_data()\n",
    "X=X_list[1]\n",
    "predicts=predict_list[1]\n",
    "overlayPredicts(X,predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC: 0.895, PR-AUC: 0.115\n",
      "Valid ROC-AUC: 0.734, PR-AUC: 0.015\n",
      "Test ROC-AUC: 0.744, PR-AUC: 0.013\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "# roc_auc_score(X.flatten(),predicts.flatten())\n",
    "m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59999\n",
      "1148666\n",
      "3818\n",
      "1148565\n",
      "85\n",
      "1135092\n",
      "8\n",
      "229319\n",
      "480\n",
      "1148694\n",
      "14\n",
      "253333\n",
      "1\n",
      "1107390\n",
      "168\n",
      "1133680\n",
      "4158\n",
      "1148249\n",
      "2604\n",
      "1148535\n",
      "14944\n",
      "1147728\n",
      "9\n",
      "1112351\n",
      "68\n",
      "1037528\n",
      "38\n",
      "1144790\n",
      "28\n",
      "1096395\n",
      "509\n",
      "1139908\n",
      "14\n",
      "1132056\n",
      "1830\n",
      "1145003\n"
     ]
    }
   ],
   "source": [
    "X=read_spike_train('/home/honglei/projects/neural_network/data/hc_data/ec012ec.227.st')\n",
    "visualSeries(X[:10000,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
